{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-13T17:53:02.357569Z","iopub.status.busy":"2023-04-13T17:53:02.357094Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mlem\n","  Downloading mlem-0.4.12-py3-none-any.whl (218 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.12.0+cpu)\n","Collecting torchvision\n","  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\n","Collecting tensorflow\n","  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.11.0+cpu)\n","Collecting torch\n","  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m836.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.11.0+cpu)\n","Collecting torchaudio\n","  Downloading torchaudio-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlem) (0.4)\n","Requirement already satisfied: click<8.2 in /opt/conda/lib/python3.7/site-packages (from mlem) (8.1.3)\n","Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from mlem) (1.5.2)\n","Collecting python-multipart\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from mlem) (2.28.1)\n","Requirement already satisfied: gitpython in /opt/conda/lib/python3.7/site-packages (from mlem) (3.1.27)\n","Requirement already satisfied: Jinja2>=3 in /opt/conda/lib/python3.7/site-packages (from mlem) (3.1.2)\n","Collecting mlem\n","  Downloading mlem-0.4.10-py3-none-any.whl (218 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.1/218.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.9-py3-none-any.whl (216 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.8-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.4/215.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.7-py3-none-any.whl (214 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.8/214.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.6-py3-none-any.whl (214 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.5-py3-none-any.whl (214 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.4-py3-none-any.whl (213 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.8/213.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.3-py3-none-any.whl (213 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec<2023.1,>=2021.7.0 in /opt/conda/lib/python3.7/site-packages (from mlem) (2022.11.0)\n","  Downloading mlem-0.4.2-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.1-py3-none-any.whl (205 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.8/205.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.4.0-py3-none-any.whl (205 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.3.2-py3-none-any.whl (197 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/197.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading mlem-0.3.1-py3-none-any.whl (189 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting iterative-telemetry\n","  Downloading iterative_telemetry-0.0.5-py3-none-any.whl (9.3 kB)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.7/site-packages (from mlem) (12.6.0)\n","Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlem) (2.1.0)\n","Collecting aiohttp-swagger<2\n","  Downloading aiohttp_swagger-1.0.16-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: isort>=5.10 in /opt/conda/lib/python3.7/site-packages (from mlem) (5.11.4)\n","Collecting pydantic<2,>=1.9.0\n","  Downloading pydantic-1.10.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting flatdict\n","  Downloading flatdict-4.0.1.tar.gz (8.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: aiohttp<4 in /opt/conda/lib/python3.7/site-packages (from mlem) (3.8.1)\n","Requirement already satisfied: pyparsing<4 in /opt/conda/lib/python3.7/site-packages (from mlem) (3.0.9)\n","Collecting python-gitlab\n","  Downloading python_gitlab-3.14.0-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer in /opt/conda/lib/python3.7/site-packages (from mlem) (0.4.2)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from mlem) (0.3.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.1.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision) (4.1.1)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install mlem torchvision tensorflow numpy torch torchaudio --upgrade\n","!pip install mlem==0.4.6 --no-deps\n","!pip install iterative-telemetry==0.0.7 --ignore-requires-python --no-deps\n","!pip install pydantic==1.10.2 --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from __future__ import print_function\n","from __future__ import division\n","\n","import copy\n","import typing\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","from collections import OrderedDict\n","\n","# Нейронки\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchvision import io as torch_io\n","from torchvision import models as torch_models\n","\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Параметры"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class WorkingMode:\n","    TRAIN: str = \"train\"\n","    VAL: str = \"val\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["INPUT_DIR: Path = Path(\"/kaggle/input/\")\n","OUTPUT_DIR: Path = Path(\"/kaggle/working/\")\n","\n","FINAL_DATASET_DIR: Path = INPUT_DIR / \"art-price\" / \"dataset\"\n","WORKING_MODES: typing.List[WorkingMode] = [WorkingMode.TRAIN, WorkingMode.VAL]\n","\n","DATASET_DIRS: typing.Dict[WorkingMode, Path] = {\n","    mode: FINAL_DATASET_DIR / mode\n","    for mode in WORKING_MODES\n","}\n","ANNOTATIONS_PATHS: typing.Dict[WorkingMode, Path] = {\n","    mode: FINAL_DATASET_DIR / f\"{mode}.csv\"\n","    for mode in WORKING_MODES\n","}\n","    \n","BATCH_SIZE: int = 64\n","N_WORKERS: int = 4\n","MODELS_DIR: Path = OUTPUT_DIR / \"models\"\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","MODEL_CLASS: typing.Type[nn.Module] = torch_models.efficientnet_b3\n","MODELS_WEIGHTS = torch_models.EfficientNet_B3_Weights.IMAGENET1K_V1\n","\n","# Detect if we have a GPU available\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["# Создание модели pytorch"]},{"cell_type":"markdown","metadata":{},"source":["Выбрал модель классификации изображений https://pytorch.org/vision/stable/models.html#:~:text=EfficientNet_B3_Weights.IMAGENET1K_V1 как оптимальную по соотношению качество/скорость работы."]},{"cell_type":"markdown","metadata":{},"source":["## Проверим модель"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict_by_model(img_path: Path, model: nn.Module, transforms=None) -> torch.Tensor:\n","    img = torch_io.read_image(str(img_path), mode=torch_io.image.ImageReadMode.RGB)\n","    if transforms:\n","        img = transforms(img)\n","    \n","    batch = img.unsqueeze(0)\n","    \n","    model.eval()\n","    prediction = model(batch).squeeze(0)\n","    \n","    return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img_path: Path = DATASET_DIRS[WorkingMode.TRAIN] / \"19.png\"\n","\n","# Initialize model with the best available weights and the inference transforms\n","model: nn.Module = MODEL_CLASS(weights=MODELS_WEIGHTS)\n","transforms = MODELS_WEIGHTS.transforms()\n","\n","prediction: torch.Tensor = predict_by_model(img_path=img_path, model=model, transforms=transforms)\n","probs: torch.Tensor = prediction.softmax(0)\n","\n","# Use the model and print the predicted category\n","class_id: int = prediction.argmax().item()\n","score = probs[class_id].item()\n","category_name = MODELS_WEIGHTS.meta[\"categories\"][class_id]\n","print(f\"{category_name}: {100 * score:.1f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["## Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ImagePricingDataset(Dataset):\n","\n","    def __init__(self, csv_path: Path, root_dir: Path, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_path: Path to the csv file with annotations.\n","            root_dir: directory with all the images.\n","            transform: Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.df: pd.DataFrame = pd.read_csv(csv_path)\n","        self.root_dir: Path = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        annotation: pd.Series = self.df.iloc[idx]\n","        \n","        img_path: Path = self.root_dir / annotation[\"image\"]\n","        image = torch_io.read_image(str(img_path), mode=torch_io.image.ImageReadMode.RGB)\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        price = annotation[\"price\"]\n","        return {'image': image, 'price': torch.tensor(price)}"]},{"cell_type":"markdown","metadata":{},"source":["### Посмотрим на примеры (проверим ImagePricingDataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_images: int = 5\n","tmp_dataset: ImagePricingDataset = ImagePricingDataset(\n","    csv_path=ANNOTATIONS_PATHS[WorkingMode.TRAIN], \n","    root_dir=DATASET_DIRS[WorkingMode.TRAIN], \n","    transform=None,\n",")\n","    \n","fig = plt.figure()\n","for i in range(len(tmp_dataset)):\n","    sample = tmp_dataset[i]\n","\n","    print(i, sample['price'], sample['image'].shape, sample['price'].shape)\n","\n","    ax = plt.subplot(1, n_images, i + 1)\n","    plt.tight_layout()\n","    ax.set_title(f\"Sample #{i}, price={sample['price']}\")\n","    ax.axis('off')\n","    plt.imshow(sample['image'].permute(1, 2, 0))\n","\n","    if i == (n_images - 1):\n","        plt.show()\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["DataLoader(ImagePricingDataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tmp_dataset: ImagePricingDataset = ImagePricingDataset(\n","    csv_path=ANNOTATIONS_PATHS[WorkingMode.TRAIN], \n","    root_dir=DATASET_DIRS[WorkingMode.TRAIN], \n","    transform=MODELS_WEIGHTS.transforms(),\n",")\n","tmp_dataloader: DataLoader = DataLoader(tmp_dataset, batch_size=5, shuffle=False, num_workers=0)\n","\n","for i, batch in enumerate(tmp_dataloader):\n","    print(i, batch[\"image\"].size(), batch[\"price\"].size(), batch[\"price\"])\n","    if i == 2:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["## DataLoadres"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Initializing Datasets and Dataloaders...\")\n","\n","data_transforms = {mode: MODELS_WEIGHTS.transforms() for mode in WORKING_MODES}\n","\n","# Create training and validation datasets\n","image_datasets: typing.Dict[WorkingMode, ImagePricingDataset] = {\n","    mode: ImagePricingDataset(\n","        csv_path=ANNOTATIONS_PATHS[mode], \n","        root_dir=DATASET_DIRS[mode], \n","        transform=data_transforms[mode],\n","    )\n","    for mode in WORKING_MODES\n","}\n","\n","# Create training and validation dataloaders\n","dataloaders: typing.Dict[WorkingMode, DataLoader] = {\n","    mode: DataLoader(image_datasets[mode], batch_size=BATCH_SIZE, shuffle=True, num_workers=N_WORKERS) \n","    for mode in WORKING_MODES\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Создадим модель для дообучения"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def set_parameter_requires_grad(model: nn.Module, requires_grad: bool = False):\n","    for param in model.parameters():\n","        param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_model(model_class: typing.Type[nn.Module], weights, freeze_body: bool = True) -> nn.Module:\n","    model: nn.Module = model_class(weights=weights)\n","    print(f\"Orig classiffier layer (last layer): \\n{model.classifier}\\n\")\n","    \n","    if freeze_body:\n","        set_parameter_requires_grad(model=model, requires_grad=False)\n","\n","    n_in_features: int = model.classifier[1].in_features\n","    model.classifier[1] = nn.Linear(n_in_features, 1)\n","    print(f\"New classiffier layer (last layer): \\n{model.classifier}\")\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model: nn.Module = create_model(model_class=MODEL_CLASS, weights=MODELS_WEIGHTS, freeze_body=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Model saving/loading"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def save_model_weights(model: nn.Module, model_name: str, model_dir: Path) -> Path:\n","    model_dir.mkdir(parents=True, exist_ok=True)\n","    weights_path: Path = model_dir / f\"{model_name}_weights.pt\"\n","\n","    torch.save(model.state_dict(), weights_path)\n","    \n","    return weights_path\n","\n","\n","def load_from_weights(model: nn.Module, weights_path: Path) -> nn.Module:\n","    \"\"\"Only weights.\"\"\"\n","    model.load_state_dict(torch.load(weights_path))\n","    model.eval()\n","    return model\n","\n","# def save_model(model: nn.Module, model_name: str, model_dir: Path) -> typing.Tuple[Path, Path]:\n","#     model_dir.mkdir(parents=True, exist_ok=True)\n","#     weights_path: Path = model_dir / f\"{model_name}_weights.pt\"\n","#     model_path: Path = model_dir / f\"{model_name}_model.pt\"\n","\n","#     torch.save(model.state_dict(), weights_path)\n","#     torch.save(model, model_path)\n","    \n","#     return weights_path, model_path\n","\n","# def load_model(model_path: Path) -> nn.Module:\n","#     \"\"\"!!!Like pickle => use class name for loading => don't work, because we change last layer?.\"\"\"\n","#     model: nn.Module = torch.load(final_model_path)\n","#     model.eval()\n","#     return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name: str = \"dummy\"\n","model_dir: Path = MODELS_DIR / model_name\n","dummy_model: nn.Module = create_model(model_class=MODEL_CLASS, weights=MODELS_WEIGHTS, freeze_body=True)\n","weights_path = save_model_weights(dummy_model, model_name=model_name, model_dir=model_dir)\n","\n","# Load\n","new_model: nn.Module = create_model(model_class=MODEL_CLASS, weights=MODELS_WEIGHTS, freeze_body=True)\n","new_model = load_from_weights(model=new_model, weights_path=weights_path)\n","print(\"Loaded from weights\")\n","\n","\n","# # Strict model\n","# other_model: nn.Module = load_model(model_path)\n","# print(\"Loaded model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img_path: Path = DATASET_DIRS[WorkingMode.TRAIN] / \"19.png\"\n","transforms = MODELS_WEIGHTS.transforms()\n","\n","dummy_prediction: torch.Tensor = predict_by_model(img_path=img_path, model=dummy_model, transforms=transforms)\n","new_prediction: torch.Tensor = predict_by_model(img_path=img_path, model=new_model, transforms=transforms)\n","# other_prediction: torch.Tensor = predict_by_model(img_path=img_path, model=other_model, transforms=transforms)\n","\n","print(dummy_prediction, new_prediction)"]},{"cell_type":"markdown","metadata":{},"source":["### Оптимизатор и Функция потерь"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Params to learn:\")\n","params_to_update = []\n","for name, param in model.named_parameters():\n","    if param.requires_grad == True:\n","        params_to_update.append(param)\n","        print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer = optim.Adam(params_to_update, lr=0.001)\n","criterion = nn.MSELoss()"]},{"cell_type":"markdown","metadata":{},"source":["# Обучение модели"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def rmsle(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n","    return torch.mean((torch.log10(y_true) - torch.log10(y_pred))**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def run_epoch(\n","    working_mode: WorkingMode, \n","    model: nn.Module, \n","    dataloader: DataLoader, \n","    criterion, \n","    optimizer, \n","    num_epochs: int = 5\n",") -> float:\n","    \"\"\"Run one full epoch of train/val process.\"\"\"\n","    start_date: datetime.datetime = datetime.datetime.now()\n","    print(f\"{working_mode} start time: {start_date}\")\n","\n","    is_train: bool = (working_mode == WorkingMode.TRAIN)\n","    if is_train:\n","        model.train()  # Set model to training mode\n","    else:\n","        model.eval()   # Set model to evaluate mode\n","\n","    mse_running_loss: float = 0.0\n","    acummulative_rmsle_running_loss: float = 0.0\n","    dataset_size: int = len(dataloader.dataset)\n","\n","    # Iterate over data.\n","    for batch in tqdm(dataloader, total=dataset_size // dataloader.batch_size):\n","        inputs = batch[\"image\"].to(DEVICE)\n","        prices = batch[\"price\"].to(DEVICE)\n","\n","        log_prices = torch.log10(prices)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward\n","        # track history if only in train\n","        with torch.set_grad_enabled(is_train):\n","            # Get model outputs and calculate loss\n","\n","            log_pred_prices = model(inputs).reshape(-1)\n","            loss = criterion(log_pred_prices, log_prices)\n","\n","            pred_prices = torch.pow(log_pred_prices, 10).detach()\n","\n","            # backward + optimize only if in training phase\n","            if is_train:\n","                loss.backward()\n","                optimizer.step()\n","\n","        # statistics\n","        mse_running_loss += loss.item() * inputs.size(0)\n","        acummulative_rmsle_running_loss += ((torch.log10(prices) - torch.log10(pred_prices))**2).sum().item()\n","#         break\n","\n","    epoch_mse_loss: float = (mse_running_loss / dataset_size)\n","    epoch_rmsle: float = np.sqrt(acummulative_rmsle_running_loss / dataset_size)\n","\n","    end_date: datetime.datetime = datetime.datetime.now()\n","    time_delta: datetime.timedelta = end_date - start_date\n","    print(f\"Epoch complete in {time_delta}\")\n","    print(f\"{working_mode} RMSLE: {epoch_rmsle:.4f}, MSE: {epoch_mse_loss:.4f}\")\n","    \n","    return epoch_rmsle"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model: nn.Module = create_model(model_class=MODEL_CLASS, weights=MODELS_WEIGHTS, freeze_body=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["N_EPOCHES: int = 25\n","start_epoch: int = 1\n","\n","model_name: str = \"orig_dataset\"\n","model_dir: Path = MODELS_DIR / model_name"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["start_date: datetime.datetime = datetime.datetime.now()\n","print(f\"Start time: {start_date}\")\n","\n","val_rmsle_history = []\n","\n","best_model_wts = copy.deepcopy(model.state_dict())\n","best_rmsle = np.inf\n","\n","for epoch in range(start_epoch, N_EPOCHES + 1):\n","    try:\n","        print(\"-\" * 80)\n","        print(f\"Epoch {epoch}/{N_EPOCHES}\")\n","        print(\"-\" * 80)\n","\n","        # Each epoch has a training and validation phase\n","        for mode in WORKING_MODES:\n","            epoch_rmsle: float = run_epoch(\n","                working_mode=mode,\n","                model=model, \n","                dataloader=dataloaders[mode], \n","                criterion=criterion, \n","                optimizer=optimizer,\n","            )\n","\n","            # deep copy the model\n","            if mode == WorkingMode.VAL:\n","                val_rmsle_history.append(epoch_rmsle)\n","\n","                if epoch_rmsle < best_rmsle:\n","                    best_rmsle = epoch_rmsle\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                    weights_path = save_model_weights(model=model, model_name=f\"best_{model_name}\", model_dir=model_dir)\n","\n","        weights_path = save_model_weights(model=model, model_name=f\"{epoch}_{model_name}\", model_dir=model_dir)\n","\n","        end_date: datetime.datetime = datetime.datetime.now()\n","        time_delta: datetime.timedelta = end_date - start_date\n","        print(f\"Epoch complete in {time_delta}\")\n","        print(f\"Best rmsle: {best_rmsle:4f}\")\n","        print(\"\\n\")\n","        \n","    except KeyboardInterrupt:\n","        weights_path = save_model_weights(\n","            model=model, \n","            model_name=f\"INTERRUPTED_{model_name}_{datetime.datetime.now()}\", \n","            model_dir=model_dir,\n","        )\n","        print('Saved interrupted model')\n","        raise KeyboardInterrupt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from mlem.api import save\n","\n","save(\n","    model, \n","    \"/kaggle/working/models/price_keras_preprocess\", \n","    preprocess=lambda x: np.expand_dims(x[:224,:224,:3], 0),\n","    sample_data=img,\n",");"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!zip -r price_keras_preprocess.zip models/price_keras_preprocess.mlem models/price_keras_preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
